{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('../application/.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import SQLRecordManager, index\n",
    "import os\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "\n",
    "host = os.getenv(\"PG_VECTOR_HOST\")\n",
    "user = os.getenv(\"PG_VECTOR_USER\")\n",
    "password = os.getenv(\"PG_VECTOR_PASSWORD\")\n",
    "COLLECTION_NAME = os.getenv(\"PGDATABASE\")\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}:5432/{COLLECTION_NAME}\"\n",
    "\n",
    "namespace = f\"pgvector/{COLLECTION_NAME}\"\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace, db_url=CONNECTION_STRING\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index([], record_manager, vector_store, cleanup=\"full\", source_id_key=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_split_documents(chunk_size=200, chunk_overlap=20):\n",
    "    conn_str = os.getenv(\"BLOB_CONN_STRING\")\n",
    "    container_name = os.getenv(\"BLOB_CONTAINER\")\n",
    "\n",
    "    if conn_str is None or container_name is None:\n",
    "        raise ValueError(\"Environment variables for BLOB_CONN_STRING or BLOB_CONTAINER are not set.\")\n",
    "\n",
    "    loader = AzureBlobStorageContainerLoader(conn_str=conn_str, container=container_name)\n",
    "    data = loader.load()\n",
    "    for doc in data:\n",
    "        doc.metadata[\"source\"] = os.path.basename(doc.metadata[\"source\"])\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, is_separator_regex=False)\n",
    "    return text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_and_split_documents()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index(docs, record_manager, vector_store, cleanup=\"full\", source_id_key=\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now update something in the raw data and upload it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "folder_path = \"./restaurant\"\n",
    "\n",
    "conn_str=os.getenv(\"BLOB_CONN_STRING\")\n",
    "container_name = os.getenv(\"BLOB_CONTAINER\")\n",
    "\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=filename)\n",
    "\n",
    "        with open(file_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_and_split_documents()\n",
    "for doc in docs[0:3]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index(docs, record_manager, vector_store, cleanup=\"full\", source_id_key=\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have linked Blob Storage And PgVector and keep the raw docs and the indexed documents in sync. But in a real app we want to handle that automatically. Thats was Azure functions can do for us. We will take a look at Azure Functions later in the course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
