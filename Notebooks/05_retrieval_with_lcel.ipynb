{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('../application/.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.schema.messages import get_buffer_string\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import format_document\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "host = os.getenv(\"PG_VECTOR_HOST\")\n",
    "user = os.getenv(\"PG_VECTOR_USER\")\n",
    "password = os.getenv(\"PG_VECTOR_PASSWORD\")\n",
    "COLLECTION_NAME = os.getenv(\"PGDATABASE\")\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}:5432/{COLLECTION_NAME}\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "COLLECTION_NAME = \"pgvector\"\n",
    "store = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"When are the opening hours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"When are the opening hours?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import format_document\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "host = os.getenv(\"PG_VECTOR_HOST\")\n",
    "user = os.getenv(\"PG_VECTOR_USER\")\n",
    "password = os.getenv(\"PG_VECTOR_PASSWORD\")\n",
    "COLLECTION_NAME = os.getenv(\"PG_COLLECTION_NAME\")\n",
    "\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}:5432/{COLLECTION_NAME}\"\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "COLLECTION_NAME = \"pgvector\"\n",
    "store = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_question_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_question_template)\n",
    "\n",
    "answer_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "def _format_chat_history(chat_history):\n",
    "    return \"\\n\".join(f\"Human: {human}\\nAssistant: {assistant}\" for human, assistant in chat_history)\n",
    "\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Chain for retrieving documents and generating the answer\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"When does the restaurant open?\",\n",
    "        \"chat_history\": []\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_format_chat_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
