{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.schema.messages import get_buffer_string\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.indexes import SQLRecordManager\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import format_document\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CONNECTION_STRING = (\n",
    "    \"postgresql+psycopg2://codingcrashcourses:123Sence@mycodingappdatabase.postgres.database.azure.com:5432/pgvector\"\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=\"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\")\n",
    "COLLECTION_NAME = \"pgvector\"\n",
    "store = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='foo'),\n",
       " Document(page_content=\"30 minutes before the restaurant closing time. Whether you're craving a quick lunch, planning a cozy dinner at home, or simply indulging in a late-night snack, La Tavola Calda is just a chat away.\", metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'}),\n",
       " Document(page_content='just a chat away. Enjoy the ease of ordering through our chatbot, and let us take care of the rest. Buon appetito!', metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'}),\n",
       " Document(page_content=\"By simply engaging with our friendly chatbot, you can place your order at any time, and we'll ensure that your favorite pizzas are on their way to you, hot and fresh.\", metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Was gibt es zu essen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='foo'), Document(page_content=\"30 minutes before the restaurant closing time. Whether you're craving a quick lunch, planning a cozy dinner at home, or simply indulging in a late-night snack, La Tavola Calda is just a chat away.\", metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'}), Document(page_content='just a chat away. Enjoy the ease of ordering through our chatbot, and let us take care of the rest. Buon appetito!', metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'}), Document(page_content=\"By simply engaging with our friendly chatbot, you can place your order at any time, and we'll ensure that your favorite pizzas are on their way to you, hot and fresh.\", metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the context provided, it is implied that there is food available to eat. However, without further information or a menu, it is not possible to determine what specific food options are available.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    print(docs)\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n",
    "\n",
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"was gibt es f√ºr Essen?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.indexes import SQLRecordManager\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import format_document\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CONNECTION_STRING = (\n",
    "    \"postgresql+psycopg2://codingcrashcourses:123Sence@mycodingappdatabase.postgres.database.azure.com:5432/pgvector\"\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=\"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\")\n",
    "COLLECTION_NAME = \"pgvector\"\n",
    "store = PGVector(\n",
    "    collection_name=\"pgvector\",\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Define the templates\n",
    "condense_question_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_question_template)\n",
    "\n",
    "answer_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "# Function to format the chat history\n",
    "def _format_chat_history(chat_history):\n",
    "    return \"\\n\".join(f\"Human: {human}\\nAssistant: {assistant}\" for human, assistant in chat_history)\n",
    "\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    print(docs)\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "# Chain for creating standalone question\n",
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Chain for retrieving documents and generating the answer\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='the exact amount before confirming your order. Restaurant Opening Hours:', metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'}), Document(page_content='Monday to Thursday: 11:00 AM - 11:00 PM Friday: 11:00 AM - 12:00 AM (midnight) Saturday: 10:00 AM - 12:00 AM (midnight) Sunday: 10:00 AM - 11:00 PM Special Hours: Our kitchen closes 30 minutes before', metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'}), Document(page_content=\"30 minutes before the restaurant closing time. Whether you're craving a quick lunch, planning a cozy dinner at home, or simply indulging in a late-night snack, La Tavola Calda is just a chat away.\", metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'}), Document(page_content='La Tavola Calda - Delivery Service & Opening Hours', metadata={'source': '/tmp/tmpqoks3ax8/hotels/opening_hours.txt'})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The restaurant opens at 11:00 AM.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"When does the restaurant open?\",\n",
    "        \"chat_history\": []\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_format_chat_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
